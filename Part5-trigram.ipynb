{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hmm_5:\n",
    "    def __init__(self,file,k=3):\n",
    "        self.words = None #set of unique words\n",
    "        self.tag_word_count = None # dict((tag,word),count)\n",
    "        self.transmissions = None # dict((tag_u,tag_v),count)\n",
    "        self.tag2_ls = None\n",
    "        self.count = None # dict(tag,count)\n",
    "        self.uvw_dic=None\n",
    "        self.read_file(file,k)\n",
    "        self.tags = set(self.count.keys())\n",
    "        \n",
    "        for key in self.tag_word_count:\n",
    "            self.tag_word_count[key] = self.tag_word_count[key]/self.count[key[0]]\n",
    "            \n",
    "        self.word_ls = tuple(self.words)#tuple cause immutable\n",
    "        self._tag_ls = tuple(self.tags)\n",
    "        #move #START# to first row and #END# to last row\n",
    "        ls = list(self._tag_ls)\n",
    "        ls.remove('#START#')\n",
    "        ls.remove('#END#')\n",
    "        self.tag3_ls = tuple(ls)\n",
    "        ls.insert(0,'#None#')\n",
    "        ls.insert(0,'#START#')\n",
    "        ls.append('#END#')\n",
    "        self.tag_ls = tuple(ls)\n",
    "        self.make_matrix()\n",
    "    \n",
    "    def make_matrix(self):\n",
    "        tag_length = len(self.tag_ls)\n",
    "        transition_matrix = np.zeros((tag_length,tag_length))\n",
    "        self.transition_matrix = pd.DataFrame(transition_matrix,index=self.tag_ls,columns=self.tag_ls)\n",
    "        \n",
    "        transition_matrix2 = np.zeros((len(self.tag2_ls),tag_length))\n",
    "        \n",
    "        for i in range(len(self.tag2_ls)):\n",
    "            for j in range(tag_length):\n",
    "                tag_uv = self.tag2_ls[i]\n",
    "                tag_w = self.tag_ls[j]\n",
    "                \n",
    "                transition_matrix2[i][j] = self.uvw_dic[tag_uv[0],tag_uv[1],tag_w]\n",
    "                \n",
    "        self.transition_matrix2 = pd.DataFrame(transition_matrix2,index=self.tag2_ls,columns=self.tag_ls)\n",
    "        \n",
    "        word_length = len(self.word_ls)\n",
    "        em_matrix = np.zeros((tag_length,word_length))\n",
    "        self.em_matrix = pd.DataFrame(em_matrix,index=self.tag_ls,columns=self.word_ls)\n",
    "\n",
    "        em_matrix = np.zeros((tag_length,word_length))\n",
    "        for i in range(tag_length):\n",
    "            for j in range(word_length):\n",
    "                tag = self.tag_ls[i]\n",
    "                word = self.word_ls[j]\n",
    "                em_matrix[i][j] = self.tag_word_count[tag,word]\n",
    "        self.em_matrix_proba = pd.DataFrame(em_matrix,index=self.tag_ls,columns=self.word_ls)\n",
    "        pass\n",
    "    \n",
    "    def read_file(self,file,k):\n",
    "        from collections import defaultdict\n",
    "        seq = ['#START#']\n",
    "        f = open(file,'r',encoding='UTF-8')\n",
    "        tag_word_ls = []\n",
    "        word_count = defaultdict(int)\n",
    "        for line in f:\n",
    "            split = line.split(' ')\n",
    "            if len(split)<2:\n",
    "                #this is a line break\n",
    "                seq.append('#END#')\n",
    "                seq.append('#START#')\n",
    "                continue\n",
    "            word,tag = split\n",
    "            word = word.strip()\n",
    "            tag = tag.strip()\n",
    "            tag_word_ls.append([tag,word])\n",
    "            word_count[word]+=1\n",
    "            seq.append(tag)\n",
    "        f.close()\n",
    "        \n",
    "        #Emissions\n",
    "        for i in range(len(tag_word_ls)):\n",
    "            tag,word = tag_word_ls[i]\n",
    "            if word_count[word]<k:\n",
    "                tag_word_ls[i] = [tag,'#UNK#']\n",
    "        tag_word_count = defaultdict(int)\n",
    "        \n",
    "        words = []\n",
    "        for tag,word in tag_word_ls:\n",
    "            tag_word_count[tag,word]+=1\n",
    "            words.append(word)\n",
    "        self.words = set(words)\n",
    "        self.tag_word_count= tag_word_count\n",
    "        \n",
    "        #Transitions\n",
    "        del seq[-1] #delete last item from the list\n",
    "         #print(seq)\n",
    "        trans_dict = defaultdict(int)\n",
    "        count_u = defaultdict(int)\n",
    "        for i in range(len(seq)-1):\n",
    "            tag_u = seq[i]\n",
    "            count_u[tag_u] += 1 # need to count #END# too\n",
    "            if tag_u == \"#END#\":\n",
    "                continue\n",
    "            #if u is not #END# we count the transmission \n",
    "            tag_v = seq[i+1]\n",
    "            if (tag_u ==\"#START#\" and tag_v == \"#END#\"):\n",
    "                #check for empty blank lines at the end and dont count them\n",
    "                print('these are blank lines')\n",
    "                count_u[\"#START#\"] -= 1 #remove additional start\n",
    "                break\n",
    "            trans_dict[(tag_u,tag_v)] += 1\n",
    "        self.transmissions = trans_dict\n",
    "        self.count = count_u\n",
    "        \n",
    "        uvw_dic = defaultdict(int)\n",
    "        uv_dic = defaultdict(int)\n",
    "        uvw_dic[(\"#None#\",seq[0],seq[1])] += 1\n",
    "        for i in range(2,len(seq),1):\n",
    "            if seq[i-2] == '#END#':\n",
    "                tag_uvw = (\"#None#\",seq[i-1],seq[i])\n",
    "                uvw_dic[tag_uvw] += 1\n",
    "                tag_uv = (\"#None#\",seq[i-1])\n",
    "                uv_dic[tag_uv] += 1\n",
    "            if seq[i-1] != '#END#' and seq[i-2] != '#END#':\n",
    "                tag_uvw = (seq[i-2],seq[i-1],seq[i])\n",
    "                uvw_dic[tag_uvw] += 1 \n",
    "            if seq[i-2] != '#END#':\n",
    "                tag_uv = (seq[i-2],seq[i-1])\n",
    "                uv_dic[tag_uv] += 1\n",
    "        \n",
    "        for key in uvw_dic:\n",
    "            uvw_dic[key] = uvw_dic[key]/uv_dic[(key[0],key[1])]\n",
    "        \n",
    "        self.uvw_dic = uvw_dic\n",
    "            \n",
    "        uvw_dic = defaultdict(int)\n",
    "        uvw_dic[\"#None#\",'#START#']=0\n",
    "        tag_pair = set(self.count.keys())\n",
    "        tag_pair.add('#None#')\n",
    "        for comb in product(tag_pair, repeat=2):\n",
    "            uvw_dic[comb[0],comb[1]]=0\n",
    "        self.tag2_ls = list(uvw_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log(m):\n",
    "    m = np.clip(m, 1e-32, None)\n",
    "    x = np.log(m)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vertebi_5(word_arr,Hmm):\n",
    "    \"\"\"\n",
    "    Followed pseudocode here\n",
    "    https://en.wikipedia.org/wiki/Viterbi_algorithm#Pseudocode\n",
    "    \"\"\"\n",
    "    states = Hmm.tag_ls[:-1] #set of all possible tags remove #START# and #STOP#\n",
    "    S = Hmm.tag2_ls\n",
    "    tag_u ,tag_v = zip(*S)\n",
    "    A = Hmm.transition_matrix2 # A(tag_uv_vector,tag_w)\n",
    "    B = Hmm.em_matrix_proba # B(tag_u->word)\n",
    "    T = len(S) # Total number unique tags\n",
    "    N = len(word_arr)+2 # Length of sentence make sure no #START# and #STOP#\n",
    "    \n",
    "    T1 = pd.DataFrame(index=S, columns=range(N)).fillna(float('-inf')) # score \n",
    "    T2 = pd.DataFrame(index=S, columns=range(N)) # backpointer\n",
    "    T1.loc[(\"#None#\",'#START#'), 0] = 1 #initialization\n",
    "    \n",
    "    #iterate through each word\n",
    "    for j in range(1,N-1):\n",
    "        word = word_arr[j-1]\n",
    "        if word not in Hmm.words:\n",
    "            word = '#UNK#'\n",
    "        x = (log(A.multiply(B[word],axis='columns'))).add(T1[j-1],axis='index').astype('float64')\n",
    "        \n",
    "        #iterate through each possible tag except #END#\n",
    "        for curr_tag in states:\n",
    "            uv=S[np.argmax(x[curr_tag].values)] #Top u,v -> w\n",
    "            score = np.max(x[curr_tag].values)  #get score from top u,v\n",
    "            T1.loc[[(uv[1],curr_tag)],j] = score #store score in T1(v,w)\n",
    "            T2.loc[[(uv[1],curr_tag)],j] = uv[0] #store u in T2(v,w)\n",
    "    \n",
    "    #handle #END#\n",
    "    j = N-1\n",
    "    x = log(A['#END#']).add(T1[j-1],axis='index').astype('float64')\n",
    "    best_pair = x.idxmax() #u,v -> end\n",
    "    T1[j].loc[(best_pair[1],'#END#')] = x.max()\n",
    "    T2[j].loc[(best_pair[1],'#END#')] = best_pair[0]\n",
    "    \n",
    "    #backtrack\n",
    "    pair = T1[N-1].astype('float64').idxmax()\n",
    "    ans = []\n",
    "    for i in range(N-1,0,-1):\n",
    "        next_state = T2.loc[[pair],i][0]\n",
    "        ans.append(pair[1])\n",
    "        pair = (next_state, pair[0])\n",
    "    ans = ans[::-1][:-1] #reverse and remove #END#\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(name,hmm_5):\n",
    "    file_object = open(\"./\"+name+\"/dev.in\", \"r\")\n",
    "    ls=[[]]\n",
    "    index=0\n",
    "    test=[]\n",
    "    for line in file_object:\n",
    "        test.append(line.strip())\n",
    "        if (line.strip()==\"\"):\n",
    "            ls.append([])\n",
    "            index+=1\n",
    "        else:\n",
    "            ls[index].append(line.strip())\n",
    "    ls.pop(-1)\n",
    "    test_df = pd.DataFrame(test, columns = ['Word'])\n",
    "    predict=[]\n",
    "#     c=0\n",
    "    for i in tqdm(ls):\n",
    "#         print(c+1)\n",
    "        out = vertebi_5(i,hmm_5)\n",
    "        for j in out:\n",
    "            predict.append(j)\n",
    "        predict.append(\"\")\n",
    "#         c+=1\n",
    "    test_df['Tag'] = predict\n",
    "    test_df.to_csv(\"./\"+name+\"/dev.p5.out\", sep=\" \", index=False, header=False)\n",
    "    file_object.close()\n",
    "#     print(c)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7aff7b05876446fc8a7bffc54b99a825",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1492), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33f75ebd0ae94b148b084c332b5c4f73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1094), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "names = ['AL','EN']\n",
    "for name in names:\n",
    "    hmm = Hmm_5('./'+name+'/train')\n",
    "    test(name,hmm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "#Entity in gold data: 13179\r\n",
      "#Entity in prediction: 13049\r\n",
      "\r\n",
      "#Correct Entity : 11118\r\n",
      "Entity  precision: 0.8520\r\n",
      "Entity  recall: 0.8436\r\n",
      "Entity  F: 0.8478\r\n",
      "\r\n",
      "#Correct Sentiment : 10773\r\n",
      "Sentiment  precision: 0.8256\r\n",
      "Sentiment  recall: 0.8174\r\n",
      "Sentiment  F: 0.8215\r\n"
     ]
    }
   ],
   "source": [
    "!python3 ./EvalScript/evalResult.py ./EN/dev.out ./EN/dev.p5.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "#Entity in gold data: 8408\r\n",
      "#Entity in prediction: 8427\r\n",
      "\r\n",
      "#Correct Entity : 7080\r\n",
      "Entity  precision: 0.8402\r\n",
      "Entity  recall: 0.8421\r\n",
      "Entity  F: 0.8411\r\n",
      "\r\n",
      "#Correct Sentiment : 6393\r\n",
      "Sentiment  precision: 0.7586\r\n",
      "Sentiment  recall: 0.7603\r\n",
      "Sentiment  F: 0.7595\r\n"
     ]
    }
   ],
   "source": [
    "!python3 ./EvalScript/evalResult.py ./AL/dev.out ./AL/dev.p5.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_out(name,hmm_5):\n",
    "    file_object = open(\"./Test/\"+name+\"/test.in\", \"r\")\n",
    "    ls=[[]]\n",
    "    index=0\n",
    "    test=[]\n",
    "    for line in file_object:\n",
    "        test.append(line.strip())\n",
    "        if (line.strip()==\"\"):\n",
    "            ls.append([])\n",
    "            index+=1\n",
    "        else:\n",
    "            ls[index].append(line.strip())\n",
    "    ls.pop(-1)\n",
    "    test_df = pd.DataFrame(test, columns = ['Word'])\n",
    "    predict=[]\n",
    "\n",
    "    for i in tqdm(ls):\n",
    "        out = vertebi_5(i,hmm_5)\n",
    "        for j in out:\n",
    "            predict.append(j)\n",
    "        predict.append(\"\")\n",
    "    test_df['Tag'] = predict\n",
    "    test_df.to_csv(\"./Test/\"+name+\"/test.p5.out\", sep=\" \", index=False, header=False)\n",
    "    file_object.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7c7972071f64752a6ff761308337392",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2985), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5a2bfa0f0b14f228ccd5a6d3ea3e598",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2189), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "names = ['AL','EN']\n",
    "for name in names:\n",
    "    hmm = Hmm_5('./'+name+'/train')\n",
    "    test_out(name,hmm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
