{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Emission:\n",
    "    def __init__(self,file,k=3):\n",
    "        self.df = None\n",
    "        self.k = k\n",
    "        self.count_series = None\n",
    "        self.read_file(file)\n",
    "        pass\n",
    "    def read_file(self,file):\n",
    "        file_object = open(file, \"r\",encoding='UTF-8')\n",
    "        dataset=[]\n",
    "        for line in file_object:\n",
    "            dataset.append(line)\n",
    "        data=[]\n",
    "        for i in range(len(dataset)):\n",
    "            temp = dataset[i].split()\n",
    "            if (len(temp)==2):\n",
    "                data.append(temp)\n",
    "        self.df = self.smoothen(pd.DataFrame(data, columns = ['Word', 'Tag']),self.k)\n",
    "        self.count_series = self.df.groupby(['Word', 'Tag']).size()\n",
    "        tag_count = self.df['Tag'].value_counts()\n",
    "        self.count_series = self.count_series / (pd.Series(self.count_series.index.get_level_values('Tag').tolist(), index=self.count_series.index).apply(lambda x: tag_count[x]))\n",
    "        file_object.close()\n",
    "    def emission(self,tag,word):\n",
    "        df = self.df\n",
    "        is_tag = (df.Tag==tag)\n",
    "        y = is_tag.sum()\n",
    "        xy = (df.Word[is_tag]==word).sum()\n",
    "        return xy/y\n",
    "    @staticmethod\n",
    "    def smoothen(df,k):\n",
    "        ls = df['Word'].value_counts()[df['Word'].value_counts()<k].index.tolist()\n",
    "        df.loc[df['Word'].isin(ls), 'Word'] = \"#UNK#\"\n",
    "        return df\n",
    "    def argmax(self,word):\n",
    "        \"\"\"\n",
    "        This part not needed in part 3?\n",
    "        produces the tag with highest probability for the word in the sequence.\n",
    "        \"\"\"\n",
    "        if (word==\"\"):\n",
    "            return (\"\",None)\n",
    "        try:\n",
    "            value = self.count_series.loc[word].idxmax()\n",
    "            probability = self.count_series.loc[word].max()\n",
    "        except:\n",
    "            value = self.count_series.loc[\"#UNK#\"].idxmax()\n",
    "            probability = self.count_series.loc[\"#UNK#\"].max()\n",
    "        return (value, probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03707354471277586\n",
      "('B-VP', 0.03707354471277586)\n"
     ]
    }
   ],
   "source": [
    "EN_Emission = Emission('./EN/train')\n",
    "print(EN_Emission.emission('B-VP','are'))\n",
    "print(EN_Emission.argmax('are'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transmission:    \n",
    "    def __init__(self,file):\n",
    "        self.transmissions = None #dictionary where KEY is a tuple (tag_u,tag_v) where tag_u -> tag_v and VALUE = count\n",
    "        self.count = None #dictionary where KEY is the tag and VALUE = count\n",
    "        self.read_file(file)\n",
    "    def read_file(self,file):\n",
    "        \"\"\"\n",
    "        read training file and returns 2 dictionaries\n",
    "        RETURNS trans_dict,count_u\n",
    "        trans_dict - > dictionary where KEY is a tuple (tag_u,tag_v) where tag_u _> tag_v and VALUE = count\n",
    "        count_u -> KEY is the tag and VALUE = count\n",
    "        \"\"\"\n",
    "        from collections import defaultdict\n",
    "        seq = ['#START#']\n",
    "        f = open(file,'r',encoding='UTF-8')\n",
    "        for line in f:\n",
    "            split = line.split(' ')\n",
    "            if len(split)<2:\n",
    "                #this is a line break\n",
    "                seq.append('#END#')\n",
    "                seq.append('#START#')\n",
    "                continue\n",
    "            word,tag = split\n",
    "            word = word.strip()\n",
    "            tag = tag.strip()\n",
    "            seq.append(tag)\n",
    "        f.close()\n",
    "        del seq[-1] #delete last item from the list\n",
    "#         print(seq)\n",
    "        trans_dict = defaultdict(int)\n",
    "        count_u = defaultdict(int)\n",
    "        for i in range(len(seq)-1):\n",
    "            tag_u = seq[i]\n",
    "            count_u[tag_u] += 1 # need to count #END# too\n",
    "            if tag_u == \"#END#\":\n",
    "                continue\n",
    "            #if u is not #END# we count the transmission \n",
    "            tag_v = seq[i+1]\n",
    "            if (tag_u ==\"#START#\" and tag_v == \"#END#\"):\n",
    "                #check for empty blank lines at the end and dont count them\n",
    "                print('these are blank lines')\n",
    "                count_u[\"#START#\"] -= 1 #remove additional start\n",
    "                break\n",
    "            trans_dict[(tag_u,tag_v)] += 1\n",
    "        self.transmissions = trans_dict\n",
    "        self.count = count_u\n",
    "        return trans_dict,count_u\n",
    "    def transmission_proba(self,tag_u,tag_v):\n",
    "        \"\"\"\n",
    "        Auv = Count(tag_u->tag_v)/Count(tag_u)\n",
    "        \"\"\"\n",
    "        return self.transmissions[(tag_u,tag_v)]/self.count[tag_u]\n",
    "    \n",
    "    def vector_proba(self,ls,tag_v):\n",
    "        #lazy calculation instead of using transistion matrix(in slides)\n",
    "        \"\"\"\n",
    "        Returns vectorized formed when a list of tag_u is given \n",
    "        \"\"\"\n",
    "        ans = []\n",
    "        for word in ls:\n",
    "            ans.append(self.transmission_proba(word,tag_v))\n",
    "        return np.array(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6480490669450607\n",
      "0.00023253355882042067\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "EN_transmission = Transmission('./EN/train')\n",
    "print(EN_transmission.transmission_proba('#START#','B-NP'))\n",
    "print(EN_transmission.transmission_proba('B-NP','#END#'))\n",
    "print(EN_transmission.transmission_proba('#START#','#START#'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vertebi(word_arr,Transmission,Emission):\n",
    "    \"\"\"\n",
    "    Followed pseudocode here\n",
    "    https://en.wikipedia.org/wiki/Viterbi_algorithm#Pseudocode\n",
    "    \"\"\"\n",
    "    S = list(Transmission.count.keys()) #set of all possible tags remove #START# and #STOP#\n",
    "    S.remove('#START#')\n",
    "    S.remove('#END#')\n",
    "    A = Transmission.vector_proba # A(tag_u_vector,tag_v)\n",
    "    B = Emission.emission # B(tag_u->word)\n",
    "    T = len(S) # Total number unique tags\n",
    "    N = len(word_arr) # Length of sentence make sure no #START# and #STOP#\n",
    "    \n",
    "    T1 = np.zeros((T,N)) #probability table of most possible path to node i.e. store scores of each node\n",
    "    T2 = np.zeros((T,N)) # Table of paths where the ith row stores highest scoring paths to T1[i,j]\n",
    "    \n",
    "    #Handle first word and base case at the same time\n",
    "    for i in range(T):\n",
    "        T1[i,0] = 1 * Transmission.transmission_proba('#START#',S[i]) * B(S[i],word_arr[0])\n",
    "        T2[i,0] = 0 #Path for first column is set to 0 same for all\n",
    "    #Note A is vector operation\n",
    "    # Fill up each column by using previous column\n",
    "    # j is position of word\n",
    "    for j in range(1,N):\n",
    "        # i is position of tag\n",
    "        #ignore #START# and #END# tag when looping\n",
    "        for i in range(T):\n",
    "            tag = S[i]\n",
    "            #note A(S,tag_u gives a vector)\n",
    "            T1[i][j] = np.max(T1[:,j-1]*A(S,tag)*B(tag,word_arr[j])) \n",
    "            T2[i][j] = np.argmax(T1[:,j-1]*A(S,tag))\n",
    "    #handle last word to #END#\n",
    "    #no emission of #END# \n",
    "    print(T1)\n",
    "    print(T2)\n",
    "    print(T1[:,N-1]*A(S,'#END#'))\n",
    "    best_row = np.argmax(np.max(T1[:,N-1]*A(S,'#END#')))\n",
    "    for j in range(N):\n",
    "        index = T2[best_row][j]\n",
    "        print(S[j])\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = ['The','dog','is','a','good','boy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.71927191e-02 0.00000000e+00 0.00000000e+00 1.48963834e-10\n",
      "  1.54697816e-15 0.00000000e+00]\n",
      " [0.00000000e+00 6.46917165e-07 0.00000000e+00 0.00000000e+00\n",
      "  6.72614449e-14 1.50320387e-18]\n",
      " [0.00000000e+00 0.00000000e+00 5.68751796e-09 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 3.82073006e-13\n",
      "  3.00693417e-15 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  7.45082543e-16 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [2.37685071e-05 0.00000000e+00 4.92833916e-11 2.24851354e-13\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 3.75360952e-16\n",
      "  0.00000000e+00 0.00000000e+00]]\n",
      "[[0. 0. 1. 2. 0. 1.]\n",
      " [0. 0. 1. 0. 0. 1.]\n",
      " [0. 0. 1. 2. 0. 1.]\n",
      " [0. 0. 1. 2. 0. 1.]\n",
      " [0. 0. 1. 2. 0. 1.]\n",
      " [0. 0. 0. 0. 4. 4.]\n",
      " [0. 0. 1. 2. 0. 1.]\n",
      " [0. 0. 1. 2. 0. 1.]\n",
      " [0. 0. 1. 2. 0. 1.]\n",
      " [0. 0. 0. 2. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 2. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 2. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 7. 0. 2. 7. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 7. 0. 7. 7. 0.]]\n",
      "[0.00000000e+00 1.18403705e-21 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00]\n",
      "B-NP\n",
      "I-NP\n",
      "B-VP\n",
      "B-ADVP\n",
      "B-ADJP\n",
      "I-ADJP\n"
     ]
    }
   ],
   "source": [
    "vertebi(word,EN_transmission,EN_Emission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EN_Emission.emission('O',\"Trump\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Word     Tag   \n",
       "!        O         0.000628\n",
       "#        B-ADJP    0.001713\n",
       "         B-NP      0.000423\n",
       "         I-NP      0.000183\n",
       "         O         0.000042\n",
       "                     ...   \n",
       "young    I-NP      0.000128\n",
       "younger  B-NP      0.000085\n",
       "         I-NP      0.000092\n",
       "your     B-NP      0.000803\n",
       "         I-NP      0.000037\n",
       "Length: 12171, dtype: float64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EN_Emission.count_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['He', 'ran', 'home']\n"
     ]
    }
   ],
   "source": [
    "print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1,2,3,4,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 4]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[1:-1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
